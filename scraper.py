"""
scraper.py â€” çŸ¥ä¹å†…å®¹çˆ¬è™«æ ¸å¿ƒæ¨¡å—

åŠŸèƒ½ï¼š
1. ä½¿ç”¨ Playwright æŒä¹…åŒ–ä¸Šä¸‹æ–‡ç™»å½•çŸ¥ä¹ï¼ˆæ‰‹åŠ¨ç™»å½•ï¼Œä¿å­˜ Cookieï¼‰
2. çˆ¬å–æŒ‡å®šç”¨æˆ·çš„æ‰€æœ‰å›ç­”å’Œæ–‡ç« é“¾æ¥
3. çˆ¬å–æŒ‡å®šé—®é¢˜ä¸‹çš„æ‰€æœ‰ï¼ˆæˆ–å‰ N ä¸ªï¼‰å›ç­”
4. çˆ¬å–å•ä¸ªå›ç­”ï¼Œå¯é€‰é™„å¸¦è¯„è®ºåŒº
5. é€ä¸ªè®¿é—®å¹¶æå–å†…å®¹ï¼Œè½¬ä¸º Markdown ä¿å­˜
6. å†…ç½®åæ£€æµ‹ï¼ˆstealth JS æ³¨å…¥ã€æŒ‡çº¹ä¼ªè£…ï¼‰
7. è¯·æ±‚é—´éš”éšæœºå»¶è¿Ÿï¼Œé™ä½è¢«å°é£é™©
"""

import asyncio
import hashlib
import json
import random
import re
import time
from datetime import date as dt_date, datetime
from pathlib import Path
from urllib.parse import urlparse

import httpx
from playwright.async_api import async_playwright, Page, BrowserContext

from stealth import STEALTH_JS
from converter import ZhihuConverter

# â”€â”€ é…ç½® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

USER_DATA_DIR = Path(__file__).parent / "browser_data"
DEFAULT_OUTPUT_DIR = Path(__file__).parent / "output"

USER_AGENT = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
    "AppleWebKit/537.36 (KHTML, like Gecko) "
    "Chrome/128.0.0.0 Safari/537.36"
)

IMG_HEADERS = {
    "Referer": "https://www.zhihu.com/",
    "User-Agent": USER_AGENT,
}

# æ¯æ¬¡è¯·æ±‚ä¹‹é—´çš„å»¶è¿ŸèŒƒå›´ï¼ˆç§’ï¼‰
MIN_DELAY = 5
MAX_DELAY = 10


# â”€â”€ å·¥å…·å‡½æ•° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def sanitize_filename(name: str) -> str:
    """æ¸…ç†æ–‡ä»¶åä¸­ä¸å…è®¸çš„å­—ç¬¦ã€‚"""
    name = re.sub(r'[/\\:*?"<>|\x00-\x1f]', "_", name)
    name = name.strip(" .")
    if len(name) > 120:
        name = name[:120].rstrip(" .")
    return name or "untitled"


def random_delay():
    """è¿”å›ä¸€ä¸ªéšæœºå»¶è¿Ÿæ—¶é—´ã€‚"""
    return MIN_DELAY + random.random() * (MAX_DELAY - MIN_DELAY)


def _nested_get(d: dict, *keys):
    """å®‰å…¨åœ°ä»åµŒå¥—å­—å…¸ä¸­è·å–å€¼ã€‚"""
    for key in keys:
        if isinstance(d, dict):
            d = d.get(key, {})
        else:
            return None
    return d if d != {} else None


def parse_question_id(input_str: str) -> str:
    """ä» URL æˆ–çº¯æ•°å­—ä¸­æå–é—®é¢˜ IDã€‚"""
    match = re.search(r'question/(\d+)', input_str)
    if match:
        return match.group(1)
    if input_str.strip().isdigit():
        return input_str.strip()
    raise ValueError(f"æ— æ³•è¯†åˆ«é—®é¢˜ ID: {input_str}")


def parse_answer_url(input_str: str) -> tuple[str, str, str]:
    """
    ä» URL ä¸­æå–ä¿¡æ¯ï¼Œè¿”å› (å®Œæ•´ URL, é—®é¢˜ ID, å›ç­” ID)ã€‚

    æ”¯æŒæ ¼å¼:
        https://www.zhihu.com/question/12345/answer/67890
        /question/12345/answer/67890
    """
    match = re.search(r'question/(\d+)/answer/(\d+)', input_str)
    if match:
        qid, aid = match.group(1), match.group(2)
        full_url = f"https://www.zhihu.com/question/{qid}/answer/{aid}"
        return full_url, qid, aid
    raise ValueError(f"æ— æ³•è¯†åˆ«å›ç­” URL: {input_str}")


# â”€â”€ æµè§ˆå™¨ä¸Šä¸‹æ–‡ç®¡ç† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def create_browser_context(pw, headless=False) -> BrowserContext:
    """åˆ›å»ºå¸¦æœ‰åæ£€æµ‹çš„æŒä¹…åŒ–æµè§ˆå™¨ä¸Šä¸‹æ–‡ã€‚"""
    USER_DATA_DIR.mkdir(parents=True, exist_ok=True)

    width = 1920 + random.randint(-100, 100)
    height = 1080 + random.randint(-50, 50)

    launch_args = [
        "--no-first-run",
        "--no-default-browser-check",
        "--disable-blink-features=AutomationControlled",
        "--disable-features=IsolateOrigins,site-per-process",
        "--disable-site-isolation-trials",
        "--disable-infobars",
        f"--window-size={width},{height}",
    ]

    context = await pw.chromium.launch_persistent_context(
        user_data_dir=str(USER_DATA_DIR),
        headless=headless,
        slow_mo=50,
        args=launch_args,
        viewport={"width": width, "height": height},
        user_agent=USER_AGENT,
        locale="zh-CN",
        timezone_id="Asia/Shanghai",
        java_script_enabled=True,
    )

    # æ³¨å…¥åæ£€æµ‹è„šæœ¬
    await context.add_init_script(STEALTH_JS)

    return context


# â”€â”€ ç™»å½• â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def login(timeout: int = 300):
    """
    æ‰“å¼€çŸ¥ä¹ç™»å½•é¡µé¢ï¼Œç­‰å¾…ç”¨æˆ·æ‰‹åŠ¨ç™»å½•ã€‚
    ç™»å½•çŠ¶æ€ä¼šä¿å­˜åœ¨ browser_data ç›®å½•ä¸­ï¼Œåç»­çˆ¬å–æ— éœ€é‡å¤ç™»å½•ã€‚

    Args:
        timeout: ç­‰å¾…ç™»å½•çš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ 300 ç§’
    """
    print("=" * 60)
    print("ğŸ” çŸ¥ä¹ç™»å½•")
    print("=" * 60)
    print(f"å°†æ‰“å¼€æµè§ˆå™¨ï¼Œè¯·åœ¨ {timeout} ç§’å†…å®Œæˆç™»å½•ã€‚")
    print("ç™»å½•æˆåŠŸåï¼Œç¨‹åºä¼šè‡ªåŠ¨æ£€æµ‹å¹¶ä¿å­˜ç™»å½•çŠ¶æ€ã€‚\n")

    async with async_playwright() as pw:
        context = await create_browser_context(pw, headless=False)
        try:
            page = context.pages[0] if context.pages else await context.new_page()
            await page.goto("https://www.zhihu.com/signin", wait_until="domcontentloaded")

            print("â³ ç­‰å¾…ç™»å½•... è¯·åœ¨æµè§ˆå™¨ä¸­å®Œæˆç™»å½•æ“ä½œã€‚")

            # ç­‰å¾…ç”¨æˆ·ç™»å½•æˆåŠŸï¼ˆæ£€æµ‹è·³è½¬åˆ°é¦–é¡µæˆ–å‡ºç°ç”¨æˆ·å¤´åƒï¼‰
            start_time = time.time()
            while time.time() - start_time < timeout:
                url = page.url
                # ç™»å½•æˆåŠŸåä¸€èˆ¬ä¼šè·³è½¬åˆ°é¦–é¡µ
                if "signin" not in url and "signup" not in url:
                    # é¢å¤–ç­‰å¾…å‡ ç§’ç¡®ä¿ Cookie å®Œå…¨å†™å…¥
                    await asyncio.sleep(3)
                    print("âœ… ç™»å½•æˆåŠŸï¼ç™»å½•çŠ¶æ€å·²ä¿å­˜ã€‚")
                    print(f"   æ•°æ®ç›®å½•: {USER_DATA_DIR.resolve()}")
                    return True
                await asyncio.sleep(2)

            print("âŒ ç™»å½•è¶…æ—¶ï¼Œè¯·é‡è¯•ã€‚")
            return False

        finally:
            await context.close()


# â”€â”€ æ”¶é›†ç”¨æˆ·å›ç­”/æ–‡ç« åˆ—è¡¨ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def _scroll_and_collect_links(
    page: Page, base_url: str, css_selector: str, url_filter_keywords: list[str]
) -> list[str]:
    """
    åœ¨ç”¨æˆ·çš„å›ç­”/æ–‡ç« åˆ—è¡¨é¡µé¢ä¸­ä¸æ–­å‘ä¸‹æ»šåŠ¨ï¼Œæ”¶é›†æ‰€æœ‰å†…å®¹é“¾æ¥ã€‚

    Args:
        page: Playwright é¡µé¢å¯¹è±¡
        base_url: ç”¨æˆ·å›ç­”æˆ–æ–‡ç« é¡µé¢ URL
        css_selector: ç”¨äºå®šä½é“¾æ¥å…ƒç´ çš„ CSS é€‰æ‹©å™¨
        url_filter_keywords: ç”¨äºè¿‡æ»¤æœ‰æ•ˆé“¾æ¥çš„å…³é”®è¯åˆ—è¡¨ï¼ˆåŒ¹é…ä»»ä¸€å³å¯ï¼‰

    Returns:
        å»é‡åçš„é“¾æ¥åˆ—è¡¨
    """
    print(f"ğŸŒ è®¿é—®: {base_url}")
    await page.goto(base_url, wait_until="domcontentloaded")
    await asyncio.sleep(5)

    # å…³é—­å¯èƒ½çš„ç™»å½•å¼¹çª—
    await _dismiss_popup(page)

    collected_links = set()
    no_new_count = 0
    max_no_new = 10  # è¿ç»­ 10 æ¬¡æ»šåŠ¨æ²¡æœ‰æ–°é“¾æ¥åˆ™è®¤ä¸ºåˆ°åº•äº†

    scroll_count = 0
    prev_scroll_height = 0

    while no_new_count < max_no_new:
        # ä½¿ç”¨ CSS é€‰æ‹©å™¨æå–é“¾æ¥ï¼ˆæ¯” JS æ­£åˆ™æ›´å¯é ï¼‰
        link_elements = await page.query_selector_all(css_selector)
        links = []
        for el in link_elements:
            href = await el.get_attribute("href")
            if href:
                # å¤„ç†ä¸åŒæ ¼å¼çš„ URL
                if href.startswith("//"):
                    href = "https:" + href
                elif href.startswith("/"):
                    href = "https://www.zhihu.com" + href
                elif not href.startswith("http"):
                    href = "https://www.zhihu.com/" + href
                if any(kw in href for kw in url_filter_keywords):
                    links.append(href.split("?")[0])

        prev_count = len(collected_links)
        collected_links.update(links)

        new_count = len(collected_links) - prev_count
        if new_count == 0:
            no_new_count += 1
        else:
            no_new_count = 0

        scroll_count += 1
        print(f"   ğŸ“œ ç¬¬ {scroll_count} æ¬¡æ»šåŠ¨ï¼Œå·²å‘ç° {len(collected_links)} ä¸ªé“¾æ¥"
              + (f"ï¼ˆæ–°å¢ {new_count}ï¼‰" if new_count > 0 else "ï¼ˆæ— æ–°å¢ï¼‰"))

        # æ£€æŸ¥é¡µé¢æ˜¯å¦åŒ…å«æ˜ç¡®çš„"åˆ°åº•"æ ‡è¯†
        end_marker = await page.evaluate("""() => {
            const bodyText = document.body.innerText;
            return bodyText.includes('å·²æ˜¾ç¤ºå…¨éƒ¨') || bodyText.includes('æ²¡æœ‰æ›´å¤šäº†');
        }""")

        if end_marker and no_new_count >= 3:
            print("   ğŸ“‹ å·²åˆ°è¾¾åˆ—è¡¨åº•éƒ¨ï¼ˆé¡µé¢æç¤ºå·²æ˜¾ç¤ºå…¨éƒ¨ï¼‰ã€‚")
            break

        # æ£€æŸ¥é¡µé¢é«˜åº¦æ˜¯å¦è¿˜åœ¨å¢é•¿ï¼ˆæ‡’åŠ è½½æ˜¯å¦è¿˜åœ¨å·¥ä½œï¼‰
        current_scroll_height = await page.evaluate("document.body.scrollHeight")
        height_changed = current_scroll_height != prev_scroll_height
        prev_scroll_height = current_scroll_height

        # åªæœ‰åœ¨é¡µé¢é«˜åº¦ä¸å†å˜åŒ–ä¸”è¿ç»­å¤šæ¬¡æ— æ–°é“¾æ¥æ—¶æ‰è®¤ä¸ºåˆ°åº•
        if not height_changed and no_new_count >= 5:
            print("   ğŸ“‹ é¡µé¢ä¸å†åŠ è½½æ–°å†…å®¹ï¼Œåœæ­¢æ»šåŠ¨ã€‚")
            break

        # æ»šåŠ¨ â€” ä½¿ç”¨å¤šç§æ–¹å¼è§¦å‘çŸ¥ä¹çš„æ‡’åŠ è½½
        # window.scrollBy æ— æ³•è§¦å‘çŸ¥ä¹çš„ scroll äº‹ä»¶ç›‘å¬å™¨ï¼Œ
        # å¿…é¡»ä½¿ç”¨é”®ç›˜ End é”®æˆ–ç›´æ¥æ“ä½œ documentElement.scrollTop
        scroll_distance = random.randint(800, 1500)
        await page.keyboard.press("End")
        await asyncio.sleep(0.5)
        await page.evaluate(f"document.documentElement.scrollTop += {scroll_distance}")
        await asyncio.sleep(0.3)
        await page.keyboard.press("End")

        # ç­‰å¾…æ–°å†…å®¹åŠ è½½
        await asyncio.sleep(2.0 + random.random() * 2)
        # é¢å¤–ç­‰å¾…ï¼šå¦‚æœä¸Šæ¬¡æ²¡æœ‰æ–°é“¾æ¥ï¼Œå¤šç­‰ä¸€ä¼šè®©æ‡’åŠ è½½æœ‰æ—¶é—´å®Œæˆ
        if new_count == 0:
            await asyncio.sleep(2.0)

    return sorted(collected_links)


async def collect_user_answers(page: Page, user_url_token: str) -> list[str]:
    """æ”¶é›†ç”¨æˆ·çš„æ‰€æœ‰å›ç­”é“¾æ¥ã€‚"""
    url = f"https://www.zhihu.com/people/{user_url_token}/answers"
    # ä½¿ç”¨ CSS é€‰æ‹©å™¨å®šä½å›ç­”æ ‡é¢˜é“¾æ¥
    css_selector = ".ContentItem h2 a, .AnswerItem h2 a, h2.ContentItem-title a"
    return await _scroll_and_collect_links(page, url, css_selector, ["/answer/"])


async def collect_user_articles(page: Page, user_url_token: str) -> list[str]:
    """æ”¶é›†ç”¨æˆ·çš„æ‰€æœ‰æ–‡ç« é“¾æ¥ã€‚"""
    url = f"https://www.zhihu.com/people/{user_url_token}/posts"
    # ä½¿ç”¨ CSS é€‰æ‹©å™¨å®šä½æ–‡ç« æ ‡é¢˜é“¾æ¥
    css_selector = ".ContentItem h2 a, .ArticleItem h2 a, h2.ContentItem-title a"
    return await _scroll_and_collect_links(page, url, css_selector, ["zhuanlan", "/p/"])


async def collect_question_answer_links(
    page: Page, question_id: str, max_answers: int | None = None
) -> list[str]:
    """
    åœ¨é—®é¢˜é¡µé¢ä¸­æ»šåŠ¨ï¼Œæ”¶é›†å›ç­”é“¾æ¥ã€‚

    Args:
        page: Playwright é¡µé¢å¯¹è±¡
        question_id: çŸ¥ä¹é—®é¢˜ ID
        max_answers: æœ€å¤šæ”¶é›†çš„å›ç­”æ•°é‡ï¼ˆNone è¡¨ç¤ºå…¨éƒ¨ï¼‰

    Returns:
        å»é‡åçš„å›ç­”é“¾æ¥åˆ—è¡¨
    """
    url = f"https://www.zhihu.com/question/{question_id}"
    print(f"ğŸŒ è®¿é—®: {url}")
    await page.goto(url, wait_until="domcontentloaded")
    await asyncio.sleep(5)

    # å…³é—­å¯èƒ½çš„ç™»å½•å¼¹çª—
    await _dismiss_popup(page)

    collected_links = set()
    no_new_count = 0
    max_no_new = 10
    scroll_count = 0
    prev_scroll_height = 0

    while no_new_count < max_no_new:
        # ä½¿ç”¨ CSS é€‰æ‹©å™¨æå–å›ç­”é“¾æ¥
        link_elements = await page.query_selector_all('a[href*="/answer/"]')
        links = []
        for el in link_elements:
            href = await el.get_attribute("href")
            if href:
                if href.startswith("//"):
                    href = "https:" + href
                elif href.startswith("/"):
                    href = "https://www.zhihu.com" + href
                elif not href.startswith("http"):
                    href = "https://www.zhihu.com/" + href
                if f"/question/{question_id}/answer/" in href:
                    links.append(href.split("?")[0])

        prev_count = len(collected_links)
        collected_links.update(links)
        new_count = len(collected_links) - prev_count

        if new_count == 0:
            no_new_count += 1
        else:
            no_new_count = 0

        scroll_count += 1
        print(f"   ğŸ“œ ç¬¬ {scroll_count} æ¬¡æ»šåŠ¨ï¼Œå·²å‘ç° {len(collected_links)} ä¸ªå›ç­”é“¾æ¥"
              + (f"ï¼ˆæ–°å¢ {new_count}ï¼‰" if new_count > 0 else "ï¼ˆæ— æ–°å¢ï¼‰"))

        # æ£€æŸ¥æ˜¯å¦å·²è¾¾åˆ°ç›®æ ‡æ•°é‡
        if max_answers and len(collected_links) >= max_answers:
            print(f"   ğŸ“‹ å·²è¾¾åˆ°ç›®æ ‡æ•°é‡ {max_answers}ã€‚")
            break

        # æ£€æŸ¥é¡µé¢æ˜¯å¦åŒ…å«æ˜ç¡®çš„"åˆ°åº•"æ ‡è¯†
        end_marker = await page.evaluate("""() => {
            const bodyText = document.body.innerText;
            return bodyText.includes('å·²æ˜¾ç¤ºå…¨éƒ¨') || bodyText.includes('æ²¡æœ‰æ›´å¤šäº†');
        }""")

        if end_marker and no_new_count >= 3:
            print("   ğŸ“‹ å·²åˆ°è¾¾åˆ—è¡¨åº•éƒ¨ï¼ˆé¡µé¢æç¤ºå·²æ˜¾ç¤ºå…¨éƒ¨ï¼‰ã€‚")
            break

        # æ£€æŸ¥é¡µé¢é«˜åº¦æ˜¯å¦è¿˜åœ¨å¢é•¿
        current_scroll_height = await page.evaluate("document.body.scrollHeight")
        height_changed = current_scroll_height != prev_scroll_height
        prev_scroll_height = current_scroll_height

        if not height_changed and no_new_count >= 5:
            print("   ğŸ“‹ é¡µé¢ä¸å†åŠ è½½æ–°å†…å®¹ï¼Œåœæ­¢æ»šåŠ¨ã€‚")
            break

        # æ»šåŠ¨ â€” ä½¿ç”¨å¤šç§æ–¹å¼è§¦å‘çŸ¥ä¹çš„æ‡’åŠ è½½
        scroll_distance = random.randint(800, 1500)
        await page.keyboard.press("End")
        await asyncio.sleep(0.5)
        await page.evaluate(f"document.documentElement.scrollTop += {scroll_distance}")
        await asyncio.sleep(0.3)
        await page.keyboard.press("End")

        await asyncio.sleep(2.0 + random.random() * 2)
        if new_count == 0:
            await asyncio.sleep(2.0)

    result = sorted(collected_links)
    if max_answers:
        result = result[:max_answers]
    return result


# â”€â”€ é¡µé¢å†…å®¹æå– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def _dismiss_popup(page: Page) -> None:
    """å…³é—­ç™»å½•å¼¹çª—ã€‚"""
    try:
        btn = page.locator("button.Modal-closeButton")
        if await btn.count() > 0:
            await btn.click(timeout=2000)
            await page.wait_for_timeout(500)
    except Exception:
        pass


async def _safe_text(page: Page, selector: str, default: str) -> str:
    """å®‰å…¨è·å–å…ƒç´ æ–‡æœ¬ã€‚"""
    try:
        el = page.locator(selector).first
        return await el.inner_text(timeout=3000)
    except Exception:
        return default


async def _extract_date(page: Page) -> str:
    """æå–å‘å¸ƒæ—¥æœŸã€‚"""
    try:
        meta = await page.locator('meta[itemprop="datePublished"]').get_attribute(
            "content", timeout=2000
        )
        if meta:
            return meta[:10]
    except Exception:
        pass
    # å°è¯•ä»é¡µé¢å†…å®¹ä¸­æå–æ—¥æœŸ
    try:
        date_text = await _safe_text(page, ".ContentItem-time", "")
        if not date_text:
            date_text = await _safe_text(page, ".Post-Header .ContentItem-time", "")
        match = re.search(r"(\d{4}-\d{2}-\d{2})", date_text)
        if match:
            return match.group(1)
    except Exception:
        pass
    return dt_date.today().isoformat()


async def extract_answer(page: Page, url: str) -> dict:
    """
    æå–çŸ¥ä¹å›ç­”å†…å®¹ã€‚

    Returns:
        {"title": str, "author": str, "html": str, "date": str, "type": "answer", "url": str}
    """
    await page.goto(url, wait_until="domcontentloaded")
    await page.wait_for_timeout(3000)
    await _dismiss_popup(page)

    # æ£€æŸ¥åçˆ¬
    text = await page.locator("body").inner_text()
    if "40362" in text or "è¯·æ±‚å­˜åœ¨å¼‚å¸¸" in text:
        raise Exception(f"è§¦å‘çŸ¥ä¹åçˆ¬ (40362): {url}")

    # ç­‰å¾…å†…å®¹åŠ è½½
    try:
        await page.wait_for_selector(".QuestionAnswer-content, .AnswerCard", timeout=15000)
    except Exception:
        # æœ‰æ—¶å€™é¡µé¢ç»“æ„ä¸åŒï¼Œå°è¯•ç­‰å¾… RichText
        await page.wait_for_selector(".RichText", timeout=10000)

    # ç‚¹å‡»"é˜…è¯»å…¨æ–‡"
    try:
        read_more = page.locator("button:has-text('é˜…è¯»å…¨æ–‡')").first
        if await read_more.count() > 0:
            await read_more.click()
            await page.wait_for_timeout(1000)
    except Exception:
        pass

    title = await _safe_text(page, "h1.QuestionHeader-title", "æœªçŸ¥é—®é¢˜")
    author = await _safe_text(page, ".AuthorInfo-name .UserLink-link", "æœªçŸ¥ä½œè€…")
    if author == "æœªçŸ¥ä½œè€…":
        author = await _safe_text(page, ".AuthorInfo span.UserLink-Name", "æœªçŸ¥ä½œè€…")
    date = await _extract_date(page)

    # æå–å›ç­” HTML
    html = ""
    try:
        html = await page.locator(".QuestionAnswer-content .RichText").first.inner_html()
    except Exception:
        try:
            html = await page.locator(".RichText").first.inner_html()
        except Exception:
            html = await page.locator("body").inner_html()

    return {
        "title": title.strip(),
        "author": author.strip(),
        "html": html,
        "date": date,
        "type": "answer",
        "url": url,
    }


async def extract_article(page: Page, url: str) -> dict:
    """
    æå–çŸ¥ä¹ä¸“æ æ–‡ç« å†…å®¹ã€‚

    Returns:
        {"title": str, "author": str, "html": str, "date": str, "type": "article", "url": str}
    """
    await page.goto(url, wait_until="domcontentloaded")
    await page.wait_for_timeout(3000)
    await _dismiss_popup(page)

    text = await page.locator("body").inner_text()
    if "40362" in text or "è¯·æ±‚å­˜åœ¨å¼‚å¸¸" in text:
        raise Exception(f"è§¦å‘çŸ¥ä¹åçˆ¬ (40362): {url}")

    try:
        await page.wait_for_selector("h1.Post-Title", timeout=15000)
    except Exception:
        await page.wait_for_selector(".RichText", timeout=10000)

    title = await _safe_text(page, "h1.Post-Title", "æœªçŸ¥æ ‡é¢˜")
    author = await _safe_text(page, ".AuthorInfo span.UserLink-Name", "æœªçŸ¥ä½œè€…")
    if author == "æœªçŸ¥ä½œè€…":
        author = await _safe_text(page, ".AuthorInfo-name .UserLink-link", "æœªçŸ¥ä½œè€…")
    date = await _extract_date(page)

    html = ""
    try:
        rich = page.locator(".Post-RichTextContainer .RichText").first
        if await rich.count() > 0:
            html = await rich.inner_html()
        else:
            html = await page.locator(".RichText").first.inner_html()
    except Exception:
        html = await page.locator("body").inner_html()

    # å°è¯•è·å–å¤´å›¾
    try:
        title_img = page.locator("img.TitleImage").first
        if await title_img.count() > 0:
            src = await title_img.get_attribute("src")
            if src:
                html = f'<img src="{src}" alt="TitleImage"><br>{html}'
    except Exception:
        pass

    return {
        "title": title.strip(),
        "author": author.strip(),
        "html": html,
        "date": date,
        "type": "article",
        "url": url,
    }


# â”€â”€ è¯„è®ºæå– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def _fetch_comment_page(page: Page, url: str) -> dict:
    """é€šè¿‡æµè§ˆå™¨ fetch è·å–ä¸€é¡µè¯„è®ºæ•°æ®ã€‚"""
    return await page.evaluate("""
        async (url) => {
            try {
                const resp = await fetch(url, { credentials: 'include' });
                if (!resp.ok) return { data: [], paging: { is_end: true } };
                return await resp.json();
            } catch (e) {
                return { data: [], paging: { is_end: true } };
            }
        }
    """, url)


async def extract_comments(page: Page, answer_id: str) -> list[dict]:
    """
    é€šè¿‡çŸ¥ä¹ API æå–å›ç­”ä¸‹çš„æ‰€æœ‰è¯„è®ºï¼ˆåŒ…å«å­è¯„è®ºï¼‰ã€‚

    Args:
        page: Playwright é¡µé¢å¯¹è±¡ï¼ˆå¿…é¡»åœ¨çŸ¥ä¹åŸŸåä¸‹ï¼‰
        answer_id: å›ç­” ID

    Returns:
        è¯„è®ºåˆ—è¡¨ï¼Œæ¯ä¸ªè¯„è®ºåŒ…å« author, content, created_time, like_count, child_comments
    """
    print(f"   ğŸ’¬ æ­£åœ¨è·å–è¯„è®º...")

    all_comments = []
    offset = 0
    limit = 20

    while True:
        api_url = (
            f"https://www.zhihu.com/api/v4/comment_v5/answers/{answer_id}"
            f"/root_comment?order_by=score&limit={limit}&offset={offset}"
        )
        data = await _fetch_comment_page(page, api_url)

        if not data.get("data"):
            break

        for comment in data["data"]:
            root = {
                "author": _nested_get(comment, "author", "member", "name") or "åŒ¿åç”¨æˆ·",
                "content": comment.get("content", ""),
                "created_time": comment.get("created_time", 0),
                "like_count": comment.get("like_count", 0),
                "child_comments": [],
            }

            # è·å–å­è¯„è®º
            child_count = comment.get("child_comment_count", 0)
            if child_count > 0:
                comment_id = comment.get("id", "")
                child_offset = 0
                while True:
                    child_url = (
                        f"https://www.zhihu.com/api/v4/comment_v5/comment/{comment_id}"
                        f"/child_comment?order_by=ts&limit=20&offset={child_offset}"
                    )
                    child_data = await _fetch_comment_page(page, child_url)

                    if not child_data.get("data"):
                        break

                    for child in child_data["data"]:
                        root["child_comments"].append({
                            "author": _nested_get(child, "author", "member", "name") or "åŒ¿åç”¨æˆ·",
                            "content": child.get("content", ""),
                            "created_time": child.get("created_time", 0),
                            "like_count": child.get("like_count", 0),
                            "reply_to": _nested_get(child, "reply_to_author", "member", "name") or "",
                        })

                    paging = child_data.get("paging", {})
                    if paging.get("is_end", True):
                        break
                    child_offset += 20
                    await asyncio.sleep(0.3)

            all_comments.append(root)

        paging = data.get("paging", {})
        if paging.get("is_end", True):
            break
        offset += limit
        await asyncio.sleep(0.5)

    total = len(all_comments)
    child_total = sum(len(c["child_comments"]) for c in all_comments)
    print(f"   ğŸ’¬ å…±è·å– {total} æ¡æ ¹è¯„è®ºï¼Œ{child_total} æ¡å­è¯„è®º")

    return all_comments


def format_comments_markdown(comments: list[dict]) -> str:
    """å°†è¯„è®ºæ•°æ®æ ¼å¼åŒ–ä¸º Markdown æ–‡æœ¬ã€‚"""
    if not comments:
        return ""

    lines = ["\n\n---\n", "## è¯„è®ºåŒº\n"]

    for i, comment in enumerate(comments, 1):
        ts = comment.get("created_time", 0)
        time_str = datetime.fromtimestamp(ts).strftime("%Y-%m-%d %H:%M") if ts else "æœªçŸ¥æ—¶é—´"
        author = comment.get("author", "åŒ¿åç”¨æˆ·")
        likes = comment.get("like_count", 0)
        content = comment.get("content", "")

        lines.append(f"### {i}æ¥¼ Â· {author} Â· {time_str} Â· ğŸ‘ {likes}\n")
        lines.append(f"{content}\n")

        # å­è¯„è®º
        for child in comment.get("child_comments", []):
            child_ts = child.get("created_time", 0)
            child_time = datetime.fromtimestamp(child_ts).strftime("%Y-%m-%d %H:%M") if child_ts else "æœªçŸ¥æ—¶é—´"
            child_author = child.get("author", "åŒ¿åç”¨æˆ·")
            child_likes = child.get("like_count", 0)
            child_content = child.get("content", "")
            reply_to = child.get("reply_to", "")

            reply_prefix = f"å›å¤ {reply_to} " if reply_to else ""
            lines.append(f"> **{child_author}** {reply_prefix}Â· {child_time} Â· ğŸ‘ {child_likes}  ")
            lines.append(f"> {child_content}")
            lines.append(">")

        lines.append("")

    return "\n".join(lines)


# â”€â”€ å›¾ç‰‡ä¸‹è½½ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def download_images(img_urls: list[str], dest: Path) -> dict[str, str]:
    """ä¸‹è½½å›¾ç‰‡åˆ°æœ¬åœ°ï¼Œè¿”å› URL â†’ æœ¬åœ°è·¯å¾„ çš„æ˜ å°„ã€‚"""
    dest.mkdir(parents=True, exist_ok=True)
    url_to_local: dict[str, str] = {}

    limits = httpx.Limits(max_keepalive_connections=5, max_connections=10)
    async with httpx.AsyncClient(
        headers=IMG_HEADERS,
        timeout=30.0,
        follow_redirects=True,
        limits=limits,
    ) as client:
        for img_url in img_urls:
            try:
                if img_url.startswith("//"):
                    img_url = "https:" + img_url

                if "data:image" in img_url or "equation" in img_url:
                    continue

                resp = await client.get(img_url)
                resp.raise_for_status()

                ext = Path(urlparse(img_url).path).suffix or ".jpg"
                if len(ext) > 5:
                    ext = ".jpg"

                fname = hashlib.md5(img_url.encode()).hexdigest()[:12] + ext
                fpath = dest / fname
                fpath.write_bytes(resp.content)
                url_to_local[img_url] = f"images/{fname}"
            except Exception:
                pass

    return url_to_local


# â”€â”€ ä¿å­˜å•ç¯‡å†…å®¹ä¸º Markdown â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def save_content_as_markdown(
    info: dict, output_dir: Path, download_img: bool = True,
    comments: list[dict] | None = None,
) -> Path:
    """
    å°†æå–åˆ°çš„å†…å®¹ä¿å­˜ä¸º Markdown æ–‡ä»¶ã€‚

    Args:
        info: extract_answer æˆ– extract_article è¿”å›çš„å­—å…¸
        output_dir: è¾“å‡ºæ ¹ç›®å½•
        download_img: æ˜¯å¦ä¸‹è½½å›¾ç‰‡åˆ°æœ¬åœ°
        comments: è¯„è®ºåˆ—è¡¨ï¼ˆå¯é€‰ï¼Œä¼ å…¥åˆ™è¿½åŠ è¯„è®ºåŒºï¼‰

    Returns:
        ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
    """
    title = info["title"]
    author = info["author"]
    date = info["date"]
    html = info["html"]
    content_type = info["type"]
    url = info["url"]

    type_label = "å›ç­”" if content_type == "answer" else "æ–‡ç« "
    folder_name = sanitize_filename(f"[{date}] {title} - {author}")

    # æŒ‰ç±»å‹åˆ†ç›®å½•
    type_dir = output_dir / ("answers" if content_type == "answer" else "articles")
    folder = type_dir / folder_name
    folder.mkdir(parents=True, exist_ok=True)

    # ä¸‹è½½å›¾ç‰‡
    img_map = {}
    if download_img:
        img_urls = ZhihuConverter.extract_image_urls(html)
        if img_urls:
            print(f"   ğŸ–¼ï¸  å‘ç° {len(img_urls)} å¼ å›¾ç‰‡ï¼Œæ­£åœ¨ä¸‹è½½...")
            img_dir = folder / "images"
            img_map = await download_images(img_urls, img_dir)
            print(f"   âœ… æˆåŠŸä¸‹è½½ {len(img_map)} å¼ å›¾ç‰‡")
            # æ¸…ç†ç©ºç›®å½•
            if img_dir.exists() and not any(img_dir.iterdir()):
                img_dir.rmdir()

    # HTML â†’ Markdown
    converter = ZhihuConverter(img_map=img_map)
    md = converter.convert(html)

    # æ‹¼æ¥å…ƒä¿¡æ¯å¤´
    header = (
        f"# {title}\n\n"
        f"> **ç±»å‹**: {type_label}  \n"
        f"> **ä½œè€…**: {author}  \n"
        f"> **æ¥æº**: [{url}]({url})  \n"
        f"> **æ—¥æœŸ**: {date}\n\n"
        f"---\n\n"
    )

    md_path = folder / "index.md"

    # æ‹¼æ¥è¯„è®ºåŒº
    comments_md = ""
    if comments:
        comments_md = format_comments_markdown(comments)

    md_path.write_text(header + md + comments_md, encoding="utf-8")

    return md_path


def _scan_done_urls_from_disk(output_dir: Path) -> set[str]:
    """
    æ‰«æè¾“å‡ºç›®å½•ä¸­å·²å­˜åœ¨çš„ Markdown æ–‡ä»¶ï¼Œä»æ–‡ä»¶å¤´éƒ¨æå–æ¥æº URLã€‚
    è¿™æ ·å³ä½¿ progress.json ä¸¢å¤±æˆ–ä¸å®Œæ•´ï¼Œå·²ä¸‹è½½çš„å†…å®¹ä¹Ÿä¸ä¼šè¢«é‡å¤çˆ¬å–ã€‚
    """
    done = set()
    url_pattern = re.compile(r'>\s*\*\*æ¥æº\*\*:\s*\[([^\]]+)\]')
    for subdir in ("answers", "articles"):
        type_dir = output_dir / subdir
        if not type_dir.exists():
            continue
        for md_file in type_dir.rglob("index.md"):
            try:
                # åªè¯»å‰ 500 å­—èŠ‚å³å¯ï¼ŒURL åœ¨æ–‡ä»¶å¤´éƒ¨
                text = md_file.read_text(encoding="utf-8")[:500]
                m = url_pattern.search(text)
                if m:
                    done.add(m.group(1))
            except Exception:
                pass
    return done


# â”€â”€ ä¸»çˆ¬å–æµç¨‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def scrape_user(
    user_url_token: str,
    output_dir: Path | None = None,
    scrape_answers: bool = True,
    scrape_articles: bool = True,
    download_img: bool = True,
    delay_min: float = 5.0,
    delay_max: float = 10.0,
    headless: bool = False,
):
    """
    çˆ¬å–æŒ‡å®šçŸ¥ä¹ç”¨æˆ·çš„æ‰€æœ‰å›ç­”å’Œ/æˆ–æ–‡ç« ã€‚

    Args:
        user_url_token: çŸ¥ä¹ç”¨æˆ·çš„ URL tokenï¼ˆä¸ªäººä¸»é¡µ URL ä¸­çš„æ ‡è¯†ç¬¦ï¼‰
                        ä¾‹å¦‚ https://www.zhihu.com/people/xxx ä¸­çš„ xxx
        output_dir: è¾“å‡ºç›®å½•
        scrape_answers: æ˜¯å¦çˆ¬å–å›ç­”
        scrape_articles: æ˜¯å¦çˆ¬å–æ–‡ç« 
        download_img: æ˜¯å¦ä¸‹è½½å›¾ç‰‡
        delay_min: è¯·æ±‚é—´æœ€å°å»¶è¿Ÿï¼ˆç§’ï¼‰
        delay_max: è¯·æ±‚é—´æœ€å¤§å»¶è¿Ÿï¼ˆç§’ï¼‰
        headless: æ˜¯å¦ä½¿ç”¨æ— å¤´æ¨¡å¼
    """
    global MIN_DELAY, MAX_DELAY
    MIN_DELAY = delay_min
    MAX_DELAY = delay_max

    if output_dir is None:
        output_dir = DEFAULT_OUTPUT_DIR / sanitize_filename(user_url_token)

    output_dir.mkdir(parents=True, exist_ok=True)

    print("=" * 60)
    print(f"ğŸ“š å¼€å§‹çˆ¬å–ç”¨æˆ·: {user_url_token}")
    print(f"   è¾“å‡ºç›®å½•: {output_dir.resolve()}")
    print(f"   çˆ¬å–å›ç­”: {'æ˜¯' if scrape_answers else 'å¦'}")
    print(f"   çˆ¬å–æ–‡ç« : {'æ˜¯' if scrape_articles else 'å¦'}")
    print(f"   ä¸‹è½½å›¾ç‰‡: {'æ˜¯' if download_img else 'å¦'}")
    print(f"   è¯·æ±‚å»¶è¿Ÿ: {delay_min}-{delay_max} ç§’")
    print("=" * 60)

    async with async_playwright() as pw:
        context = await create_browser_context(pw, headless=headless)

        try:
            page = context.pages[0] if context.pages else await context.new_page()

            # â”€â”€ æ”¶é›†é“¾æ¥ â”€â”€
            all_urls = []

            if scrape_answers:
                print("\nğŸ“ æ­£åœ¨æ”¶é›†å›ç­”åˆ—è¡¨...")
                answer_urls = await collect_user_answers(page, user_url_token)
                print(f"   å…±å‘ç° {len(answer_urls)} ä¸ªå›ç­”")
                all_urls.extend([(url, "answer") for url in answer_urls])

            if scrape_articles:
                print("\nğŸ“ æ­£åœ¨æ”¶é›†æ–‡ç« åˆ—è¡¨...")
                # åœ¨æ”¶é›†æ–‡ç« ä¹‹å‰æ·»åŠ å»¶è¿Ÿ
                if scrape_answers:
                    delay = random_delay()
                    print(f"   â³ ç­‰å¾… {delay:.1f} ç§’...")
                    await asyncio.sleep(delay)
                article_urls = await collect_user_articles(page, user_url_token)
                print(f"   å…±å‘ç° {len(article_urls)} ç¯‡æ–‡ç« ")
                all_urls.extend([(url, "article") for url in article_urls])

            if not all_urls:
                print("\nâš ï¸  æœªå‘ç°ä»»ä½•å†…å®¹ï¼Œè¯·æ£€æŸ¥ç”¨æˆ· URL token æ˜¯å¦æ­£ç¡®ã€‚")
                return

            total = len(all_urls)
            print(f"\nğŸš€ å…±è®¡ {total} é¡¹å†…å®¹å¾…çˆ¬å–\n")

            # â”€â”€ ä¿å­˜é“¾æ¥åˆ—è¡¨ï¼ˆç”¨äºæ–­ç‚¹ç»­ä¼ ï¼‰ â”€â”€
            links_file = output_dir / "links.json"
            links_data = [{"url": url, "type": t} for url, t in all_urls]
            links_file.write_text(
                json.dumps(links_data, ensure_ascii=False, indent=2), encoding="utf-8"
            )
            print(f"ğŸ“‹ é“¾æ¥åˆ—è¡¨å·²ä¿å­˜åˆ°: {links_file}\n")

            # â”€â”€ æ£€æŸ¥å·²çˆ¬å–çš„å†…å®¹ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰ â”€â”€
            progress_file = output_dir / "progress.json"
            done_urls = set()
            if progress_file.exists():
                try:
                    done_data = json.loads(progress_file.read_text(encoding="utf-8"))
                    done_urls = set(done_data.get("done", []))
                except Exception:
                    pass

            # æ‰«æç£ç›˜ä¸Šå·²å­˜åœ¨çš„æ–‡ä»¶ï¼Œè¡¥å…… progress.json å¯èƒ½é—æ¼çš„è®°å½•
            disk_urls = _scan_done_urls_from_disk(output_dir)
            if disk_urls - done_urls:
                print(f"ğŸ“‚ ä»ç£ç›˜æ‰«æå‘ç° {len(disk_urls - done_urls)} ä¸ªå·²ä¸‹è½½ä½†æœªè®°å½•çš„å†…å®¹")
                done_urls |= disk_urls

            if done_urls:
                # åªç»Ÿè®¡ä¸å½“å‰é“¾æ¥åˆ—è¡¨åŒ¹é…çš„æ•°é‡
                matched = sum(1 for url, _ in all_urls if url in done_urls)
                print(f"ğŸ“Œ æ£€æµ‹åˆ°ä¹‹å‰çš„è¿›åº¦ï¼Œå·²å®Œæˆ {matched}/{total} é¡¹ï¼Œå°†è·³è¿‡ã€‚\n")

                # åŒæ­¥æ›´æ–° progress.json
                progress_file.write_text(
                    json.dumps({"done": list(done_urls)}, ensure_ascii=False),
                    encoding="utf-8",
                )

            # â”€â”€ é€ä¸ªçˆ¬å– â”€â”€
            success_count = 0
            fail_count = 0

            for idx, (url, content_type) in enumerate(all_urls, 1):
                if url in done_urls:
                    print(f"[{idx}/{total}] â­ï¸  è·³è¿‡ï¼ˆå·²å®Œæˆï¼‰: {url}")
                    success_count += 1
                    continue

                print(f"[{idx}/{total}] ğŸ“¥ æ­£åœ¨çˆ¬å–{' å›ç­”' if content_type == 'answer' else 'æ–‡ç« '}: {url}")

                try:
                    if content_type == "answer":
                        info = await extract_answer(page, url)
                    else:
                        info = await extract_article(page, url)

                    md_path = await save_content_as_markdown(info, output_dir, download_img)
                    print(f"   ğŸ’¾ å·²ä¿å­˜: {md_path}")

                    success_count += 1
                    done_urls.add(url)

                    # æ›´æ–°è¿›åº¦
                    progress_file.write_text(
                        json.dumps({"done": list(done_urls)}, ensure_ascii=False),
                        encoding="utf-8",
                    )

                except Exception as e:
                    fail_count += 1
                    print(f"   âŒ å¤±è´¥: {e}")

                    # å¦‚æœè§¦å‘åçˆ¬ï¼ŒåŠ å¤§å»¶è¿Ÿ
                    if "40362" in str(e) or "åçˆ¬" in str(e):
                        extra_wait = 30 + random.random() * 30
                        print(f"   âš ï¸  è§¦å‘åçˆ¬æœºåˆ¶ï¼Œé¢å¤–ç­‰å¾… {extra_wait:.0f} ç§’...")
                        await asyncio.sleep(extra_wait)

                # è¯·æ±‚é—´å»¶è¿Ÿ
                if idx < total:
                    delay = random_delay()
                    print(f"   â³ ç­‰å¾… {delay:.1f} ç§’...\n")
                    await asyncio.sleep(delay)

            # â”€â”€ æ±‡æ€» â”€â”€
            print("\n" + "=" * 60)
            print("âœ¨ çˆ¬å–å®Œæˆï¼")
            print(f"   æˆåŠŸ: {success_count}")
            print(f"   å¤±è´¥: {fail_count}")
            print(f"   è¾“å‡ºç›®å½•: {output_dir.resolve()}")
            print("=" * 60)

        finally:
            await context.close()


async def scrape_question(
    question_input: str,
    max_answers: int | None = None,
    output_dir: Path | None = None,
    download_img: bool = True,
    delay_min: float = 10.0,
    delay_max: float = 20.0,
    headless: bool = False,
):
    """
    çˆ¬å–æŒ‡å®šçŸ¥ä¹é—®é¢˜ä¸‹çš„å›ç­”ã€‚

    Args:
        question_input: é—®é¢˜ URL æˆ–çº¯æ•°å­— ID
        max_answers: æœ€å¤šçˆ¬å–çš„å›ç­”æ•°é‡ï¼ˆNone è¡¨ç¤ºå…¨éƒ¨ï¼‰
        output_dir: è¾“å‡ºç›®å½•
        download_img: æ˜¯å¦ä¸‹è½½å›¾ç‰‡
        delay_min: è¯·æ±‚é—´æœ€å°å»¶è¿Ÿï¼ˆç§’ï¼‰
        delay_max: è¯·æ±‚é—´æœ€å¤§å»¶è¿Ÿï¼ˆç§’ï¼‰
        headless: æ˜¯å¦ä½¿ç”¨æ— å¤´æ¨¡å¼
    """
    global MIN_DELAY, MAX_DELAY
    MIN_DELAY = delay_min
    MAX_DELAY = delay_max

    question_id = parse_question_id(question_input)

    if output_dir is None:
        output_dir = DEFAULT_OUTPUT_DIR / f"question_{question_id}"
    output_dir.mkdir(parents=True, exist_ok=True)

    limit_str = f"å‰ {max_answers} ä¸ª" if max_answers else "å…¨éƒ¨"

    print("=" * 60)
    print(f"ğŸ“š å¼€å§‹çˆ¬å–é—®é¢˜: {question_id}")
    print(f"   é—®é¢˜é“¾æ¥: https://www.zhihu.com/question/{question_id}")
    print(f"   çˆ¬å–æ•°é‡: {limit_str}")
    print(f"   è¾“å‡ºç›®å½•: {output_dir.resolve()}")
    print(f"   ä¸‹è½½å›¾ç‰‡: {'æ˜¯' if download_img else 'å¦'}")
    print(f"   è¯·æ±‚å»¶è¿Ÿ: {delay_min}-{delay_max} ç§’")
    print("=" * 60)

    async with async_playwright() as pw:
        context = await create_browser_context(pw, headless=headless)

        try:
            page = context.pages[0] if context.pages else await context.new_page()

            # â”€â”€ æ”¶é›†å›ç­”é“¾æ¥ â”€â”€
            print("\nğŸ“ æ­£åœ¨æ”¶é›†å›ç­”åˆ—è¡¨...")
            answer_urls = await collect_question_answer_links(page, question_id, max_answers)
            print(f"   å…±å‘ç° {len(answer_urls)} ä¸ªå›ç­”")

            if not answer_urls:
                print("\nâš ï¸  æœªå‘ç°ä»»ä½•å›ç­”ï¼Œè¯·æ£€æŸ¥é—®é¢˜ ID æ˜¯å¦æ­£ç¡®ã€‚")
                return

            total = len(answer_urls)
            print(f"\nğŸš€ å…±è®¡ {total} ä¸ªå›ç­”å¾…çˆ¬å–\n")

            # â”€â”€ ä¿å­˜é“¾æ¥åˆ—è¡¨ â”€â”€
            links_file = output_dir / "links.json"
            links_data = [{"url": url, "type": "answer"} for url in answer_urls]
            links_file.write_text(
                json.dumps(links_data, ensure_ascii=False, indent=2), encoding="utf-8"
            )
            print(f"ğŸ“‹ é“¾æ¥åˆ—è¡¨å·²ä¿å­˜åˆ°: {links_file}\n")

            # â”€â”€ æ–­ç‚¹ç»­ä¼  â”€â”€
            progress_file = output_dir / "progress.json"
            done_urls = set()
            if progress_file.exists():
                try:
                    done_data = json.loads(progress_file.read_text(encoding="utf-8"))
                    done_urls = set(done_data.get("done", []))
                except Exception:
                    pass

            # æ‰«æç£ç›˜ä¸Šå·²å­˜åœ¨çš„æ–‡ä»¶ï¼Œè¡¥å…… progress.json å¯èƒ½é—æ¼çš„è®°å½•
            disk_urls = _scan_done_urls_from_disk(output_dir)
            if disk_urls - done_urls:
                print(f"ğŸ“‚ ä»ç£ç›˜æ‰«æå‘ç° {len(disk_urls - done_urls)} ä¸ªå·²ä¸‹è½½ä½†æœªè®°å½•çš„å†…å®¹")
                done_urls |= disk_urls

            if done_urls:
                matched = sum(1 for u in answer_urls if u in done_urls)
                print(f"ğŸ“Œ æ£€æµ‹åˆ°ä¹‹å‰çš„è¿›åº¦ï¼Œå·²å®Œæˆ {matched}/{total} é¡¹ï¼Œå°†è·³è¿‡ã€‚\n")

                progress_file.write_text(
                    json.dumps({"done": list(done_urls)}, ensure_ascii=False),
                    encoding="utf-8",
                )

            # â”€â”€ é€ä¸ªçˆ¬å– â”€â”€
            success_count = 0
            fail_count = 0

            for idx, url in enumerate(answer_urls, 1):
                if url in done_urls:
                    print(f"[{idx}/{total}] â­ï¸  è·³è¿‡ï¼ˆå·²å®Œæˆï¼‰: {url}")
                    success_count += 1
                    continue

                print(f"[{idx}/{total}] ğŸ“¥ æ­£åœ¨çˆ¬å–å›ç­”: {url}")

                try:
                    info = await extract_answer(page, url)
                    md_path = await save_content_as_markdown(info, output_dir, download_img)
                    print(f"   ğŸ’¾ å·²ä¿å­˜: {md_path}")

                    success_count += 1
                    done_urls.add(url)

                    progress_file.write_text(
                        json.dumps({"done": list(done_urls)}, ensure_ascii=False),
                        encoding="utf-8",
                    )

                except Exception as e:
                    fail_count += 1
                    print(f"   âŒ å¤±è´¥: {e}")

                    if "40362" in str(e) or "åçˆ¬" in str(e):
                        extra_wait = 30 + random.random() * 30
                        print(f"   âš ï¸  è§¦å‘åçˆ¬æœºåˆ¶ï¼Œé¢å¤–ç­‰å¾… {extra_wait:.0f} ç§’...")
                        await asyncio.sleep(extra_wait)

                if idx < total:
                    delay = random_delay()
                    print(f"   â³ ç­‰å¾… {delay:.1f} ç§’...\n")
                    await asyncio.sleep(delay)

            # â”€â”€ é—®é¢˜çˆ¬å–æ±‡æ€» â”€â”€
            print("\n" + "=" * 60)
            print("âœ¨ é—®é¢˜å›ç­”çˆ¬å–å®Œæˆï¼")
            print(f"   æˆåŠŸ: {success_count}")
            print(f"   å¤±è´¥: {fail_count}")
            print(f"   è¾“å‡ºç›®å½•: {output_dir.resolve()}")
            print("=" * 60)

        finally:
            await context.close()


async def scrape_single_answer(
    answer_input: str,
    output_dir: Path | None = None,
    download_img: bool = True,
    with_comments: bool = False,
    delay_min: float = 10.0,
    delay_max: float = 20.0,
    headless: bool = False,
):
    """
    çˆ¬å–å•ä¸ªçŸ¥ä¹å›ç­”ï¼ˆå¯é€‰é™„å¸¦è¯„è®ºåŒºï¼‰ã€‚

    Args:
        answer_input: å›ç­” URLï¼ˆåŒ…å« /question/xxx/answer/xxxï¼‰
        output_dir: è¾“å‡ºç›®å½•
        download_img: æ˜¯å¦ä¸‹è½½å›¾ç‰‡
        with_comments: æ˜¯å¦åŒæ—¶çˆ¬å–è¯„è®ºåŒº
        delay_min: è¯·æ±‚é—´æœ€å°å»¶è¿Ÿï¼ˆç§’ï¼‰
        delay_max: è¯·æ±‚é—´æœ€å¤§å»¶è¿Ÿï¼ˆç§’ï¼‰
        headless: æ˜¯å¦ä½¿ç”¨æ— å¤´æ¨¡å¼
    """
    global MIN_DELAY, MAX_DELAY
    MIN_DELAY = delay_min
    MAX_DELAY = delay_max

    answer_url, question_id, answer_id = parse_answer_url(answer_input)

    if output_dir is None:
        output_dir = DEFAULT_OUTPUT_DIR / f"answer_{answer_id}"
    output_dir.mkdir(parents=True, exist_ok=True)

    print("=" * 60)
    print(f"ğŸ“š çˆ¬å–å•ä¸ªå›ç­”")
    print(f"   å›ç­”é“¾æ¥: {answer_url}")
    print(f"   åŒ…å«è¯„è®º: {'æ˜¯' if with_comments else 'å¦'}")
    print(f"   è¾“å‡ºç›®å½•: {output_dir.resolve()}")
    print(f"   ä¸‹è½½å›¾ç‰‡: {'æ˜¯' if download_img else 'å¦'}")
    print("=" * 60)

    async with async_playwright() as pw:
        context = await create_browser_context(pw, headless=headless)

        try:
            page = context.pages[0] if context.pages else await context.new_page()

            print(f"\nğŸ“¥ æ­£åœ¨çˆ¬å–å›ç­”: {answer_url}")
            info = await extract_answer(page, answer_url)

            # è·å–è¯„è®º
            comments = None
            if with_comments:
                comments = await extract_comments(page, answer_id)

            md_path = await save_content_as_markdown(
                info, output_dir, download_img, comments=comments
            )
            print(f"   ğŸ’¾ å·²ä¿å­˜: {md_path}")

            print("\n" + "=" * 60)
            print("âœ¨ çˆ¬å–å®Œæˆï¼")
            print(f"   è¾“å‡ºç›®å½•: {output_dir.resolve()}")
            print("=" * 60)

        finally:
            await context.close()
